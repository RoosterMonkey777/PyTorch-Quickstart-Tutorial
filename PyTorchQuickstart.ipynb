{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1661eb02",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f2b81",
   "metadata": {},
   "source": [
    "Runs through the API for common tasks in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a07e22",
   "metadata": {},
   "source": [
    "### Working with data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ac1ec",
   "metadata": {},
   "source": [
    "Pytorch has two primitives to work with data:\n",
    "- torch.utils.data.DataLoader\n",
    "- torch.utils.data.Dataset\n",
    "\n",
    "Dataset stores the samples and their corresponding labels. Dataloader wraps an iterable around the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61a942cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a8df9",
   "metadata": {},
   "source": [
    "PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. \n",
    "\n",
    "For this tutorial, we will be using a TorchVision dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f15396",
   "metadata": {},
   "source": [
    "The torchvision.datasets module contains Dataset objects for many real-world vision data like CIFAR, COCO, etc (full list: https://pytorch.org/vision/stable/datasets.html). \n",
    "\n",
    "In this tutorial, we use the FashionMNIST dataset. Every TorchVision Dataset includes two arguments: transform and target_transform to modify the samples and labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f98948f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 26421880/26421880 [00:03<00:00, 7083733.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 29515/29515 [00:00<00:00, 264919.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4422102/4422102 [00:11<00:00, 375195.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 5148/5148 [00:00<00:00, 5290927.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f5805",
   "metadata": {},
   "source": [
    "Pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. \n",
    "\n",
    "Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b44609c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fffa94",
   "metadata": {},
   "source": [
    "N: This represents the number of samples or examples in the batch. In the context of image data, N would be the batch size, which is the number of images processed together during one iteration of training or inference.\n",
    "\n",
    "C: This represents the number of channels in the image. For grayscale images, C would be 1, indicating a single channel. For color images represented in RGB format, C would be 3, representing the three color channels (red, green, blue).\n",
    "\n",
    "H: This represents the height of the image in pixels.\n",
    "\n",
    "W: This represents the width of the image in pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b2daf",
   "metadata": {},
   "source": [
    "### Creating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d29625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f4a949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b41088",
   "metadata": {},
   "source": [
    "Above code defines a neural network model using the nn.Module class and prints out a summary of the model architecture.\n",
    "\n",
    "1. Define the Neural Network Class (NeuralNetwork):\n",
    "\n",
    "    The NeuralNetwork class is defined, which is a subclass of nn.Module, the base class for all neural network modules in PyTorch.\n",
    "\n",
    "    The __init__ method is called when an instance of the class is created. Inside __init__, the layers of the neural network are defined.\n",
    "    \n",
    "\n",
    "2. Define Layers in the __init__ Method:\n",
    "\n",
    "    - The __init__ method initializes the neural network layers:\n",
    "        - nn.Flatten(): This layer flattens the input tensor into a 1D tensor. It is used to flatten the input images, which are 2D (28x28 pixels), into a 1D tensor (784 pixels).\n",
    "        - nn.Sequential(): This is a container that sequentially applies a list of layers.\n",
    "        - nn.Linear(): This defines fully connected (dense) layers. It takes the size of the input and output as parameters.\n",
    "        - nn.ReLU(): This is the rectified linear unit (ReLU) activation function, applied after each linear layer except the last one.\n",
    "    - The neural network consists of three linear layers with ReLU activation functions between them.\n",
    "    - The first linear layer takes 28x28 input features (the size of the flattened image) and outputs 512 features.\n",
    "    - The second and third linear layers have 512 input features and output 512 features, and 10 features respectively.\n",
    "    - The output of the last linear layer (with 10 output features) represents the logits for each class (in this case, the FashionMNIST dataset has 10 classes).\n",
    "\n",
    "\n",
    "3. Define the forward Method:\n",
    "\n",
    "    - The forward method defines the forward pass of the neural network.\n",
    "    - It takes an input tensor x and passes it through the layers defined in the __init__ method.\n",
    "    - First, the input tensor is flattened using nn.Flatten().\n",
    "    - Then, the flattened tensor is passed through the sequential layer defined in the __init__ method.\n",
    "    - The output of the last linear layer (logits) is returned.\n",
    "    \n",
    "    \n",
    "4. Instantiate the Model and Move to Device (model = NeuralNetwork().to(device)):\n",
    "\n",
    "    - An instance of the NeuralNetwork class is created.\n",
    "    - It is then moved to the device specified earlier (CPU, GPU, or MPS) using the to method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68cc7d",
   "metadata": {},
   "source": [
    "### Optimizing the Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ebf7d9",
   "metadata": {},
   "source": [
    "To train model, need a loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7656ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22954c2f",
   "metadata": {},
   "source": [
    "In single training loop, model makes predictions on the training dataset (fed in batches), and backpropogates the prediction error to adjust the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b427a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b72ac9",
   "metadata": {},
   "source": [
    "1. Function Signature:\n",
    "    The function train takes four parameters:\n",
    "    dataloader: The PyTorch DataLoader object containing the training data.\n",
    "    model: The neural network model to be trained.\n",
    "    loss_fn: The loss function used to compute the loss.\n",
    "    optimizer: The optimizer used to update the model parameters.\n",
    "\n",
    "\n",
    "2. Set Model to Training Mode:\n",
    "    model.train() sets the model to training mode. This is important because certain layers behave differently during training and evaluation (e.g., dropout layers).\n",
    "\n",
    "\n",
    "3. Training Loop:\n",
    "    The function iterates over batches of data from the dataloader:\n",
    "\n",
    "\n",
    "4. Device Placement:\n",
    "    X, y = X.to(device), y.to(device): This line moves the input data X and target labels y to the specified device (device).\n",
    "\n",
    "\n",
    "5. Forward Pass:\n",
    "    pred = model(X): Performs a forward pass through the neural network model to compute predictions (pred) for the input data X.\n",
    "\n",
    "\n",
    "6. Compute Loss:\n",
    "    loss = loss_fn(pred, y): Computes the loss between the predicted values (pred) and the actual target labels (y) using the specified loss function (loss_fn).\n",
    "\n",
    "\n",
    "7. Backpropagation and Optimization:\n",
    "    loss.backward(): Backpropagates the gradients of the loss function with respect to the model parameters.\n",
    "    optimizer.step(): Updates the model parameters using the gradients computed during the backward pass.\n",
    "    optimizer.zero_grad(): Clears the gradients accumulated in the previous iteration. This is necessary because PyTorch accumulates gradients by default.\n",
    "\n",
    "\n",
    "8. Logging Training Progress:\n",
    "    if batch % 100 == 0: Logs the training progress every 100 batches.\n",
    "    loss, current = loss.item(), (batch + 1) * len(X): Extracts the current loss value and the current size of the processed data batch.\n",
    "    print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\"): Prints the current loss and the progress in terms of the number of samples processed out of the total dataset size.\n",
    "\n",
    "\n",
    "In summary, the train function encapsulates the training process for the neural network model. It iterates over batches of training data, computes predictions, calculates the loss, performs backpropagation, and updates the model parameters using the specified optimizer. Additionally, it logs the training progress to monitor the performance of the model during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c05e93",
   "metadata": {},
   "source": [
    "Also check the model’s performance against the test dataset to ensure it is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4595a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2303f73",
   "metadata": {},
   "source": [
    "function test responsible for evaluating the performance of a trained neural network model on a test dataset:\n",
    "\n",
    "\n",
    "1. Function Signature:\n",
    "    The function test takes three parameters:\n",
    "    dataloader: The PyTorch DataLoader object containing the test data.\n",
    "    model: The trained neural network model to be evaluated.\n",
    "    loss_fn: The loss function used to compute the loss.\n",
    "\n",
    "\n",
    "2. Initialization:\n",
    "    size = len(dataloader.dataset): Computes the total size of the test dataset.\n",
    "    num_batches = len(dataloader): Computes the number of batches in the test dataset.\n",
    "\n",
    "\n",
    "3. Set Model to Evaluation Mode:\n",
    "    model.eval(): Sets the model to evaluation mode. This disables certain layers, like dropout layers, which behave differently during training and evaluation.\n",
    "\n",
    "\n",
    "4. Initialize Variables for Evaluation:\n",
    "\n",
    "    test_loss, correct = 0, 0: Initializes variables to accumulate the total test loss and the number of correctly predicted samples.\n",
    "\n",
    "5. Evaluation Loop:\n",
    "    The function iterates over batches of data from the test dataloader.\n",
    "    with torch.no_grad(): This context manager ensures that gradients are not computed during the evaluation process. This reduces memory consumption and speeds up computation.\n",
    "\n",
    "\n",
    "6. Device Placement and Prediction:\n",
    "\n",
    "    X, y = X.to(device), y.to(device): Moves the input data X and target labels y to the specified device (device).\n",
    "    pred = model(X): Performs a forward pass through the neural network model to compute predictions (pred) for the input data X.\n",
    "\n",
    "\n",
    "7. Compute Test Loss and Accuracy:\n",
    "\n",
    "    test_loss += loss_fn(pred, y).item(): Computes the test loss for the current batch and accumulates it.\n",
    "    correct += (pred.argmax(1) == y).type(torch.float).sum().item(): Computes the number of correctly predicted samples in the current batch and accumulates it.\n",
    "\n",
    "\n",
    "8. Compute Average Test Loss and Accuracy:\n",
    "\n",
    "    test_loss /= num_batches: Computes the average test loss across all batches.\n",
    "    correct /= size: Computes the accuracy as the ratio of correctly predicted samples to the total number of samples.\n",
    "\n",
    "\n",
    "9. Print Evaluation Results:\n",
    "\n",
    "    Prints the test error, including accuracy and average loss.\n",
    "\n",
    "\n",
    "In summary, the test function evaluates the trained neural network model on a test dataset, computing the test loss and accuracy. It iterates over batches of test data, computes predictions, and accumulates metrics for evaluation. Finally, it prints out the evaluation results, including the accuracy and average loss on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35557113",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff948070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.291703  [   64/60000]\n",
      "loss: 2.291132  [ 6464/60000]\n",
      "loss: 2.264377  [12864/60000]\n",
      "loss: 2.266026  [19264/60000]\n",
      "loss: 2.250588  [25664/60000]\n",
      "loss: 2.209043  [32064/60000]\n",
      "loss: 2.234445  [38464/60000]\n",
      "loss: 2.184725  [44864/60000]\n",
      "loss: 2.185010  [51264/60000]\n",
      "loss: 2.160828  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.148675 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.151424  [   64/60000]\n",
      "loss: 2.156168  [ 6464/60000]\n",
      "loss: 2.085038  [12864/60000]\n",
      "loss: 2.106302  [19264/60000]\n",
      "loss: 2.063388  [25664/60000]\n",
      "loss: 1.989804  [32064/60000]\n",
      "loss: 2.042598  [38464/60000]\n",
      "loss: 1.946138  [44864/60000]\n",
      "loss: 1.952910  [51264/60000]\n",
      "loss: 1.897489  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 1.879922 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.905232  [   64/60000]\n",
      "loss: 1.893374  [ 6464/60000]\n",
      "loss: 1.751979  [12864/60000]\n",
      "loss: 1.800064  [19264/60000]\n",
      "loss: 1.705885  [25664/60000]\n",
      "loss: 1.640492  [32064/60000]\n",
      "loss: 1.697371  [38464/60000]\n",
      "loss: 1.574478  [44864/60000]\n",
      "loss: 1.604726  [51264/60000]\n",
      "loss: 1.517522  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.514136 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.576498  [   64/60000]\n",
      "loss: 1.555317  [ 6464/60000]\n",
      "loss: 1.382725  [12864/60000]\n",
      "loss: 1.461695  [19264/60000]\n",
      "loss: 1.350483  [25664/60000]\n",
      "loss: 1.334757  [32064/60000]\n",
      "loss: 1.382132  [38464/60000]\n",
      "loss: 1.285843  [44864/60000]\n",
      "loss: 1.327504  [51264/60000]\n",
      "loss: 1.238092  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.251544 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.326663  [   64/60000]\n",
      "loss: 1.318949  [ 6464/60000]\n",
      "loss: 1.138413  [12864/60000]\n",
      "loss: 1.243084  [19264/60000]\n",
      "loss: 1.123678  [25664/60000]\n",
      "loss: 1.143352  [32064/60000]\n",
      "loss: 1.189530  [38464/60000]\n",
      "loss: 1.110381  [44864/60000]\n",
      "loss: 1.155782  [51264/60000]\n",
      "loss: 1.077079  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.089926 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ef08e",
   "metadata": {},
   "source": [
    "### Saving Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e8732f",
   "metadata": {},
   "source": [
    "A common way to save a model is to serialize the internal state dictionary (containing the model parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc4b5d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d735d9",
   "metadata": {},
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f9d2b6",
   "metadata": {},
   "source": [
    "The process for loading a model includes re-creating the model structure and loading the state dictionary into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcf2847c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d1ce3",
   "metadata": {},
   "source": [
    "Model can now make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe540ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Pullover\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf2409",
   "metadata": {},
   "source": [
    "1. Define Classes:\n",
    "\n",
    "    The classes list contains the names of the classes corresponding to the FashionMNIST dataset. Each index in the list corresponds to a class label.\n",
    "\n",
    "\n",
    "2. Set Model to Evaluation Mode:\n",
    "\n",
    "    model.eval() sets the model to evaluation mode. This disables certain layers like dropout layers that behave differently during training and evaluation.\n",
    "\n",
    "\n",
    "3. Perform Inference:\n",
    "\n",
    "    x, y = test_data[0][0], test_data[0][1] retrieves the first sample (x) and its corresponding label (y) from the test dataset.\n",
    "    \n",
    "    with torch.no_grad(): is a context manager that disables gradient calculation during inference. This reduces memory consumption and speeds up computation.\n",
    "    \n",
    "    x = x.to(device) moves the input tensor x to the specified device (device).\n",
    "    \n",
    "    pred = model(x) performs inference by passing the input tensor through the model, obtaining the predicted output.\n",
    "    \n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y] retrieves the class names corresponding to the predicted class (determined by the index of the maximum value in the predicted tensor) and the actual class label (y).\n",
    "    \n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"') prints out the predicted class and the actual class of the sample.\n",
    "\n",
    "\n",
    "In summary, this code snippet demonstrates how to use a trained neural network model to make predictions on a single sample from the test dataset and print out the predicted class alongside the actual class. This is a common approach used to verify the performance of a trained model on unseen data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56358f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
